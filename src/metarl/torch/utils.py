# pylint: disable=global-statement
"""Utility functions for PyTorch."""
import numpy as np
import torch

device = None
_gpu_id = 0
_use_gpu = False


def np_to_torch(array_dict):
    """Convert numpy arrays to PyTorch tensors.

    Args:
        array_dict (dict): Dictionary of data in numpy arrays.

    Returns:
        dict: Dictionary of data in PyTorch tensors.

    """
    for key, value in array_dict.items():
        array_dict[key] = torch.FloatTensor(value)
    return array_dict


def torch_to_np(value_in):
    """Convert PyTorch tensors to numpy arrays.

    Args:
        value_in (tuple): Tuple of data in PyTorch tensors.

    Returns:
        tuple[numpy.ndarray]: Tuple of data in numpy arrays.

    """
    value_out = tuple(v.numpy() for v in value_in)
    return value_out


def flatten_batch(tensor):
    """Flatten a batch of observations.

    Reshape a tensor of size (X, Y, Z) into (X*Y, Z)

    Args:
        tensor (torch.Tensor): Tensor to flatten.

    Returns:
        torch.Tensor: Flattened tensor.

    """
    return tensor.reshape((-1, ) + tensor.shape[2:])


def update_module_params(module, new_params):  # noqa: D202
    """Load parameters to a module.

    This function acts like `torch.nn.Module._load_from_state_dict()`, but it
    replaces the tensors in module with those in new_params, while
    `_load_from_state_dict()` loads only the value. Use this function so that
    the `grad` and `grad_fn` of `new_params` can be restored.

    Args:
         module (torch.nn.Module): A torch module.
         new_params (dict): A dict of torch tensor used as the new parameters
            of this module. This parameters dict should be generated by
            `torch.nn.Module.named_parameters()`.

    """

    def update(m, name, param):
        del m._parameters[name]  # pylint: disable=protected-access # noqa: E501
        setattr(m, name, param)
        m._parameters[name] = param  # pylint: disable=protected-access # noqa: E501

    named_modules = dict(module.named_modules())

    for name, new_param in new_params.items():
        if '.' in name:
            module_name, param_name = tuple(name.rsplit('.', 1))
            if module_name in named_modules:
                update(named_modules[module_name], param_name, new_param)
        else:
            update(module, name, new_param)


def set_gpu_mode(mode, gpu_id=0):
    """Set GPU mode and device ID.

    Args:
        mode (bool): Whether or not to use GPU.
        gpu_id (int): GPU ID.

    """
    global device
    global _gpu_id
    global _use_gpu
    _gpu_id = gpu_id
    _use_gpu = mode
    device = torch.device(('cuda:' + str(gpu_id)) if _use_gpu else 'cpu')


def from_numpy(*args, **kwargs):
    """Convert NumPy array to Torch tensor on specified device.

    Args:
        *args (numpy.ndarray): NumPy array.

    Returns:
        torch.Tensor: Converted Torch tensor.

    """
    return torch.from_numpy(*args, **kwargs).float().to(device)


def to_numpy(tensor):
    """Convert Torch tensor to NumPy array.

    Args:
        tensor (torch.Tensor): Torch tensor.

    Returns:
        numpy.ndarray: Converted NumPy array.

    """
    return tensor.to('cpu').detach().numpy()


def zeros(*sizes, **kwargs):
    """Create a Torch tensor of zeros on specified device.

    Args:
        *sizes: Size of desired tensor.

    Returns:
        torch.Tensor: Torch tensor of zeros.

    """
    return torch.zeros(*sizes, **kwargs).to(device)


def ones(*sizes, **kwargs):
    """Create a Torch tensor of ones on specified device.

    Args:
        *sizes: Size of desired tensor.

    Returns:
        torch.Tensor: Torch tensor of ones.

    """
    return torch.ones(*sizes, **kwargs).to(device)


def elem_or_tuple_to_variable(elem_or_tuple):
    if isinstance(elem_or_tuple, tuple):
        return tuple(elem_or_tuple_to_variable(e) for e in elem_or_tuple)
    return from_numpy(elem_or_tuple).float()


def filter_batch(np_batch):
    for k, v in np_batch.items():
        if v.dtype == np.bool:
            yield k, v.astype(int)
        else:
            yield k, v


def np_to_pytorch_batch(np_batch):
    return {
        k: elem_or_tuple_to_variable(x)
        for k, x in filter_batch(np_batch) if x.dtype != np.dtype('O')
    }
